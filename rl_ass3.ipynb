{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5b401cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged after 2 iterations\n",
      "\n",
      "Value function: [2.15833333 2.15833333 2.15833333 2.15833333]\n",
      "\n",
      "\n",
      "Optimal Policy:\n",
      "[[2 3]\n",
      " [2 3]\n",
      " [1 3]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = 0.7 #probability that user picks a recommendation, given that he continues\n",
    "q = 0.3 #probability that the user exits (terminates) the session\n",
    "\n",
    "K = 4 #num of videos in platform (represents the states - numbered from 0 to K-1)\n",
    "N = 2  #num of recommendations\n",
    "u_min = 0.3 #relevance threshold\n",
    "\n",
    "U = np.random.rand(K, K) #relevance matrix\n",
    "np.fill_diagonal(U, 0)\n",
    "U_bool = U > u_min #0 if j is irrelevant to i, 1 if j is relevant to i\n",
    "\n",
    "C = np.random.randint(0, 2, size=K) #c[i] denotes wheter state i is cached or no.\n",
    "\n",
    "# for testing purposes:\n",
    "# temp = [[0, 0.8, 0, 0.8],\n",
    "#          [0.1, 0, 0.8, 0.8],\n",
    "#          [0.5, 0.7, 0, 0],\n",
    "#          [0, 0.8, 0, 0.8]]\n",
    "# U = np.array(temp)\n",
    "# temp = [0, 1, 1, 0]\n",
    "# C = np.array(temp)\n",
    "# U_bool = U > u_min #0 if j is irrelevant to i, 1 if j is relevant to i\n",
    "\n",
    "action_set = []\n",
    "for i in range(K):\n",
    "    for j in range(K):\n",
    "        if(i == j):\n",
    "            continue\n",
    "        action = [i, j]\n",
    "        action_set.append(action)\n",
    "\n",
    "\n",
    "#env model given a state and action returns p(s', r | s, a) for each (s', r) that has prob > 0.\n",
    "#Then this can be used for the Bellman update\n",
    "def environment_model(state, action): #state is an integer, action is an N-tuple of recommendations\n",
    "    scenarios = []\n",
    "    if(all_relevant(state, action)):\n",
    "        \n",
    "        prob = (1-q)*(a/N + (1-a)/K) #probability to choose any recommended video\n",
    "        for item in action:\n",
    "            reward = is_cached(item)\n",
    "            t = (prob, item, reward, False)\n",
    "            scenarios.append(t)\n",
    "        \n",
    "        prob = (1-q)*((1-a)/K) #probability to choose any not recommended video\n",
    "        for item in range(K):\n",
    "            if(item in action):\n",
    "                continue\n",
    "            reward = is_cached(item)\n",
    "            t = (prob, item, reward, False)\n",
    "            scenarios.append(t)\n",
    "            \n",
    "    else:\n",
    "        prob = (1-q)*((1-a)/K) #probability to choose any not recommended video\n",
    "        for item in range(K):\n",
    "            reward = is_cached(item)\n",
    "            t = (prob, item, reward, False)\n",
    "            scenarios.append(t)\n",
    "                            \n",
    "    t = (q, 0, 0, True) #terminal state, with probability q, terminate session\n",
    "    scenarios.append(t)\n",
    "    return scenarios\n",
    "    \n",
    "    \n",
    "def is_cached(item):\n",
    "    return C[item]\n",
    "\n",
    "def all_relevant(s, w):\n",
    "    for item in w:\n",
    "        if(not U_bool[s, item]): #if at least one of recommendations is not relevant return False\n",
    "            return False\n",
    "    return True\n",
    "        \n",
    "    \n",
    "def policy_evaluation(pi, gamma = 1.0, epsilon = 1e-10):  #inputs: (1) policy to be evaluated, (2) model of the environment (transition probabilities, etc., see previous cell), (3) discount factor (with default = 1), (4) convergence error (default = 10^{-10})\n",
    "    prev_V = np.zeros(K) # use as \"cost-to-go\", i.e. for V(s')\n",
    "    while True: #performing iterations\n",
    "        V = np.zeros(K) # current value function to be learnerd\n",
    "        for s in range(K):  # do for every state\n",
    "            for prob, next_state, reward, done in environment_model(s, pi[s, :]):  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
    "                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
    "        if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one; \n",
    "            break # if yes, finish loop\n",
    "        prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
    "    return V\n",
    "    \n",
    "    \n",
    "def policy_improvement(V, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
    "    Q = np.zeros((K, len(action_set)), dtype=np.float64) #create a Q value array\n",
    "    for s in range(K):        # for every state in the environment/model\n",
    "        for i,a in enumerate(action_set):  # and for every action in that state\n",
    "            for prob, next_state, reward, done in environment_model(s, a):  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve) \n",
    "                Q[s][i] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "#     new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]  # this basically creates the new (improved) policy by choosing at each state s the action a that has the highest Q value (based on the Q array we just calculated)\n",
    "    new_pi = generate_improved_policy(Q)\n",
    "    \n",
    "    return new_pi\n",
    "\n",
    "\n",
    "def policy_iteration(gamma = 1.0, epsilon = 1e-10):\n",
    "    t = 0\n",
    "    pi = generate_random_policy()\n",
    "    \n",
    "    while True:\n",
    "        old_pi = pi  #keep the old policy to compare with new\n",
    "        V = policy_evaluation(pi, gamma, epsilon)   #evaluate latest policy --> you receive its converged value function\n",
    "        pi = policy_improvement(V,gamma)          #get a better policy using the value function of the previous one just calculated \n",
    "        \n",
    "        t += 1\n",
    "    \n",
    "        if(np.array_equal(old_pi, pi)):# you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
    "            break\n",
    "    print('converged after %d iterations' %t) #keep track of the number of (outer) iterations to converge\n",
    "    return V,pi\n",
    "\n",
    "\n",
    "def generate_random_policy():\n",
    "    pi = np.zeros((K, N), dtype=np.int8)\n",
    "    for s in range(K):\n",
    "        action = np.random.choice(K, size=N, replace=False)\n",
    "        pi[s, :] = action\n",
    "    return pi\n",
    "\n",
    "def generate_improved_policy(Q):\n",
    "    new_pi = np.zeros((K, N), dtype=np.int8)\n",
    "    action_idxs = np.argmax(Q, axis=1) \n",
    "    for s in range(K):\n",
    "        best_action_idx = np.argmax(Q[s])\n",
    "        new_pi[s, :] = action_set[best_action_idx]\n",
    "    return new_pi\n",
    "\n",
    "\n",
    "V,pi = policy_iteration()\n",
    "print(f\"\\nValue function: {V}\\n\\n\")\n",
    "print(f\"Optimal Policy:\\n{pi}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a6aed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[3 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[3 1]\n"
     ]
    }
   ],
   "source": [
    "K = 4\n",
    "N = 2\n",
    "pi = np.zeros((K, N), dtype=np.int8)\n",
    "\n",
    "print(pi)\n",
    "a = np.random.choice(K, size=N, replace=False)\n",
    "print(type(a))\n",
    "pi[0, :] = a\n",
    "print(pi)\n",
    "print(a)\n",
    "\n",
    "\n",
    "# for row in enumerate(range(K)):\n",
    "#     action = np.random.choice(K, size=N, replace=False)\n",
    "#     pi[row, :] = action\n",
    "        \n",
    "# print(pi)\n",
    "# print('\\n')\n",
    "# print(pi[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c65fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
